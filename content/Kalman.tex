\chapter{EKF (Extended Kalman Filter)}
\section{ROS Package Dependencies}
\subsection{Robot localization}
For the functionality of the Kalman Filter the package \texttt{robot\_localization} was used. The package provides a convenient 15-dimensional implementation of the kalman filter with direct support for 6D pose, 6D velocity and 3D acceleration vector. The available inputs can be dynamically configured in the \textit{launch file} for the \texttt{robot\_localization} node. In general the whole package is configurable via launch file or yaml file.

\subsection{TF (Transformation)}
The \texttt{TF} package is used to define all sorts of data transformations in different coordinate systems. In this case the robot's map data and the input of the marker detection are converted to be fused in a common coordinate system. 

\section{Package Structure / Design}
\subsection{Configuration}
The Kalman package is highly dynamic. All properties and settings can be configured by yaml files and launch files similar to the \texttt{robot\_localization} package. So no recompilation is required if topic names are changed or the resulting frames of the data change.

\subsection{Robot Localization Node}
This node is a part of the \texttt{robot\_localization} package and implements the Extended Kalman Filter. It receives input from the topics of the Vision module INSERT NAME, the robot's map system and a third topic generated by the Kalman converter node in case the robot is kidnapped to prevent the continuous detection of kidnappings due to jumping output values.

\subsection{Converter Node}
The converter node handles the output of the \texttt{robot\_localization} node and republishes the data to the final topic INSERT NAME, the other modules are working with. This interception is used to filter all results of the robot's map data and only rely on camera data as long as the robot is in kidnapped state.

Moreover in case of a kidnapping the converter node publishes the camera results with reduced covariance as a third input topic to the Kalman Filter, to convince it that the camera data now has the only valid results. This is needed since the ranges of covariances between the different input topics vary vastly.

\section{Testing}
\subsection{Matlab Parameter Simulation}
In the beginning of the project a Matlab simulation of a Kalman Filter was created to experiment with the different parameters of a Kalman Filter and to learn how they influence estimation.

By inserting velocity data and analyzing the predictions we gained much insight into the Kalman model. Additionally the effects of the noise covariances became much clearer in a way that was very helpful throughout the project.

\subsection{Test Data Generation}
In the first months of the project it was very difficult to test the implementation since all teams started from scratch. Production and publication of sensor data was just being developed. To overcome this obstacle artificial test data was generated with two approaches.

\subsubsection{Matlab}
In Matlab test data was generated by supplying a number of points distributed in time and then using a cubic spline interpolation to generate the values in between. For simplicity this values where initially generated only for one dimension of the Kalman Filter.

\subsubsection{Simple MonoGame}
For the generation of more realistic 2D poses and view angle of the robot a small \texttt{C\#}-program was built. This program uses the MonoGame framework to allow mouse interaction on a small window. A small arrow is displayed to represent the robot. Then by clicking on a position the user can tell the robot to move there with a pre-defined fixed speed. When holding down the mouse button and releasing it on another pixel the vector between these points is used to generate the target view angle of the robot. In the background the program continuously writes the current position and view angle of the robot to a file.

\subsection{Sensor Test Data Dummy node}
Next placeholder sensor nodes were written that do nothing but read simulation data from file and publish them in a loop. This way the Kalman Filter could be tested even if the actual sensor nodes were still in development. It was now possible to tweak the many parameters of the \texttt{robot\_localization node} and study the filters behaviour independently of the other groups.

\subsection{Rviz Simulation}
After development of the input and procesing pipeline for test data the next challenge was to evaluate the filter's outputs. Since humans are very ineffective in classifying text data, the ROS package \texttt{rviz} was utilized to visually compare input data and filtered result. The visual proof for the effects of parameter changes greatly enhanced our capability to evaluate the Kalman filter's performance.

\subsection{visualization node}
The visualization node subscribes to the output topic of the Kalman Filter and the input topcis from the robot to create displayable data which can be used to verify the data fusion quality. This displayable data are markers published to \texttt{rviz} that shows them in its coordinate system. There are two possible visualization modes:

\begin{itemize}
\item Show the current 2D Pose of the robot
\item Show timely change of the robot's pose in x.
\end{itemize}

It is also possible to display process covariances over time to gain a better understanding of why there might be a sudden change in the kalman filter's behaviour.

\section{Difficulties}
\subsection{Initial start with custom Kalman Implementation}
Before using the \texttt{robot\_localization} package a quick attempt to implement the Kalman Filter manually was investigated. Luckily our attention was directed towards the \texttt{robot\_localization} package before more than data subscription and publishing were implemented, both of which were fortunately reused throughout the project.

\subsection{Difficulties with employing MCA2}
When the Kalman Filter performed satisfactorily with the artificial test data, the next step consisted of producing more realistic data with a true robot simulator. For this MCA2 appeared to be a suitable solution. But the installation of MCA2 and the simulator presented unexpected difficulties and failed in the end.

One of the reasons is the quality of the MCA2 documentation which posed a major problem. It is highly fragmented, redundant, often outdated and in some places contradictory. Very often it was difficult to understand if an installation error was caused by a user mistake or by the documentation and if the latter, whether it is only unclear, incomplete or incorrect. 

The usual approach for a computer scientist is to read the documentation and rule out any self-made errors before asking possibly unnecessary questions. This is why the lacking documentation caused more harm than good.

It was finally discovered that additional data only existing on a USB drive was required in order for the package to install successfully. But even after having received this piece of data the software could only be run exclusively on lab computers, indicating another still unknown dependency or requirement.

By that time data publishing and necessary software for it have already been facilitated on the robot. So MCA2 was abandoned and the robot itself was used to generate data.

\subsection{Roslaunch initialization and ROS\_IP behind NATs}
While working with virtual machines for ROS package development the problem of incorrect communication between the packages was discovered. Only when starting the nodes in the right order with appropriate delays in between, the system would work on the robot. In general the specification of the \texttt{ROS\_IP} environment variable is used to solve that issue. The local IP of the virtual machines behind a NAT was not available and so the execution of the nodes was completely moved to the robot.

\subsection{Wifi Overload $\Rightarrow$ huge delays}
When attaching the camera to the robot in a manner that the image data has to be sent over wifi, the whole ROS communication is extremly slow. This problem is solved by installing all drivers for the camera on the robot. So the camera and its related nodes can operate locally on the robot.

\subsection{Boundary for accepted jumps between sensor outputs}
When first configuring the \texttt{robot\_localization} package, a threshold for tolerated jumps in the sensor data was set to a rather low value. This caused the Kalman Filter to exhibit a lot of unexpected behavior, for instance it did not respect changes in one sensor at all. After setting the threshold to an appropriate value, the filter worked even better as initially expected. This is partly due to the well adjusted remaining parameters of the Kalman Filter which were already thoroughly tested with the Matlab simulation.

\subsection{Set pose of robot\_localization does not work as expected}
The \texttt{robot\_localization} package provides a \texttt{set\_pose} topic to force the actual pose of the robot. It was planned to use this topic to inform the Kalman Filter about the new pose after having recovered the robot from kidnapped state. Unfortunately this sets the pose permanently and further input is ignored, so another solution was necessary. The final approach uses a third virtual input sensor that sends the recovered pose along with very low covariances to convince the Kalman Filter to trust this source the most.

\subsection{Unclear information about the data source topics and types for the data fusion}
Finding out which scripts must be started on the robot to make it publish its pose and twist turned out to be a challenge in itself. During some periods alternative topics were used to at least receive any data to work with. For example the map position of the robot was temporarily replaced by its odometry output.